dtest <- xgb.DMatrix(data = as.matrix(x[new_samp_, ]), label = y[new_samp_])
watchlist <- list(train = dtrain, test = dtest)
param = list("objective" = "reg:linear", "bst:eta" = 0.05, "max_depth" = 6, "subsample" = 0.85,
"colsample_bytree" = 0.85, "booster" = "gbtree", "nthread" = 3)                      # "lambda" = 0.1,
fit = xgb.train(param, dtrain, nround = 500, print_every_n  = 100, watchlist = watchlist,
early_stopping_rounds = 20, maximize = FALSE, verbose = 0)
p_te = xgboost:::predict.xgb.Booster(fit, as.matrix(x[new_samp_, ]), ntreelimit = fit$best_iteration)
xgb_end = Sys.time()
highdim_xgb_time[i] = xgb_end - xgb_start
highdim_xgb_te[i] = MLmetrics::RMSE(y[new_samp_], p_te)
}
cat("total time rgf 5 fold cross-validation : ", sum(highdim_rgf_time), " mean rmse on test data : ", mean(highdim_rgf_te), "\n")
cat("total time ranger 5 fold cross-validation : ", sum(highdim_ranger_time), " mean rmse on test data : ", mean(highdim_ranger_te), "\n")
cat("total time xgb 5 fold cross-validation : ", sum(highdim_xgb_time), " mean rmse on test data : ", mean(highdim_xgb_te), "\n")
highdim_rgf_te
highdim_ranger_te
highdim_xgb_te
highdim_rgf_te = highdim_ranger_te = highdim_xgb_te = highdim_rgf_time = highdim_ranger_time = highdim_xgb_time = rep(NA, NUM_FOLDS)
for (i in 1:length(FOLDS)) {
cat("fold : ", i, "\n")
new_samp = unlist(FOLDS[-i])
new_samp_ = unlist(FOLDS[i])
# RGF
#----
rgf_start = Sys.time()
init_regr = FastRGF_Regressor$new(n_jobs = 3, l2 = 0.1)              # added 'l2' regularization
init_regr$fit(x = as.matrix(x[new_samp, ]), y = y[new_samp])
pr_te = init_regr$predict(as.matrix(x[new_samp_, ]))
rgf_end = Sys.time()
highdim_rgf_time[i] = rgf_end - rgf_start
highdim_rgf_te[i] = MLmetrics::RMSE(y[new_samp_], pr_te)
# ranger
#-------
ranger_start = Sys.time()
fit = ranger(dependent.variable.name = "Ca", data = tmp_rg_dat[new_samp, ], write.forest = TRUE, probability = F, num.threads = 3,
num.trees = 500, verbose = T, classification = F, mtry = NULL, min.node.size = 5, keep.inbag = T)
pred_te = predict(fit, data = x[new_samp_, ], type = 'se')$predictions
ranger_end = Sys.time()
highdim_ranger_time[i] = ranger_end - ranger_start
highdim_ranger_te[i] = MLmetrics::RMSE(y[new_samp_], pred_te)
# xgboost
#--------
xgb_start = Sys.time()
dtrain <- xgb.DMatrix(data = as.matrix(x[new_samp, ]), label = y[new_samp])
dtest <- xgb.DMatrix(data = as.matrix(x[new_samp_, ]), label = y[new_samp_])
watchlist <- list(train = dtrain, test = dtest)
param = list("objective" = "reg:linear", "bst:eta" = 0.05, "max_depth" = 6, "subsample" = 0.85,
"colsample_bytree" = 0.85, "booster" = "gbtree", "nthread" = 3, "lambda" = 0.1)                      # "lambda" = 0.1,
fit = xgb.train(param, dtrain, nround = 500, print_every_n  = 100, watchlist = watchlist,
early_stopping_rounds = 20, maximize = FALSE, verbose = 0)
p_te = xgboost:::predict.xgb.Booster(fit, as.matrix(x[new_samp_, ]), ntreelimit = fit$best_iteration)
xgb_end = Sys.time()
highdim_xgb_time[i] = xgb_end - xgb_start
highdim_xgb_te[i] = MLmetrics::RMSE(y[new_samp_], p_te)
}
cat("total time rgf 5 fold cross-validation : ", sum(highdim_rgf_time), " mean rmse on test data : ", mean(highdim_rgf_te), "\n")
cat("total time ranger 5 fold cross-validation : ", sum(highdim_ranger_time), " mean rmse on test data : ", mean(highdim_ranger_te), "\n")
cat("total time xgb 5 fold cross-validation : ", sum(highdim_xgb_time), " mean rmse on test data : ", mean(highdim_xgb_te), "\n")
NUM_FOLDS = 5
set.seed(1)
FOLDS = regr_folds(folds = NUM_FOLDS, Boston[, 'medv'], stratified = T)
boston_rgf_te = boston_ranger_te = boston_xgb_te = boston_rgf_time = boston_ranger_time = boston_xgb_time = rep(NA, NUM_FOLDS)
for (i in 1:length(FOLDS)) {
cat("fold : ", i, "\n")
samp = unlist(FOLDS[-i])
samp_ = unlist(FOLDS[i])
# RGF
#----
rgf_start = Sys.time()
init_regr = RGF_Regressor$new(l2 = 0.1)
init_regr$fit(x = as.matrix(Boston[samp, -ncol(Boston)]), y = Boston[samp, ncol(Boston)])
pr_te = init_regr$predict(as.matrix(Boston[samp_, -ncol(Boston)]))
rgf_end = Sys.time()
boston_rgf_time[i] = rgf_end - rgf_start
boston_rgf_te[i] = MLmetrics::RMSE(Boston[samp_, 'medv'], pr_te)
# ranger
#-------
ranger_start = Sys.time()
fit = ranger(dependent.variable.name = "medv", data = Boston[samp, ], write.forest = TRUE, probability = F, num.threads = 1,
num.trees = 500, verbose = T, classification = F, mtry = 3, min.node.size = 5, keep.inbag = T)
pred_te = predict(fit, data = Boston[samp_, -ncol(Boston)], type = 'se')$predictions
ranger_end = Sys.time()
boston_ranger_time[i] = ranger_end - ranger_start
boston_ranger_te[i] = MLmetrics::RMSE(Boston[samp_, 'medv'], pred_te)
# xgboost
#--------
xgb_start = Sys.time()
dtrain <- xgb.DMatrix(data = as.matrix(Boston[samp, -ncol(Boston)]), label = Boston[samp, ncol(Boston)])
dtest <- xgb.DMatrix(data = as.matrix(Boston[samp_, -ncol(Boston)]), label = Boston[samp_, ncol(Boston)])
watchlist <- list(train = dtrain, test = dtest)
param = list("objective" = "reg:linear", "bst:eta" = 0.05, "max_depth" = 4, "subsample" = 0.85,
"colsample_bytree" = 0.85, "booster" = "gbtree",  "nthread" = 1)
fit = xgb.train(param, dtrain, nround = 500, print_every_n  = 100, watchlist = watchlist,
early_stopping_rounds = 20, maximize = FALSE, verbose = 0)
p_te = xgboost:::predict.xgb.Booster(fit, as.matrix(Boston[samp_, -ncol(Boston)]), ntreelimit = fit$best_iteration)
xgb_end = Sys.time()
boston_xgb_time[i] = xgb_end - xgb_start
boston_xgb_te[i] = MLmetrics::RMSE(Boston[samp_, 'medv'], p_te)
}
# time and error-rate
cat("total time rgf 5 fold cross-validation : ", sum(boston_rgf_time), " mean rmse on test data : ", mean(boston_rgf_te), "\n")
cat("total time ranger 5 fold cross-validation : ", sum(boston_ranger_time), " mean rmse on test data : ", mean(boston_ranger_te), "\n")
cat("total time xgb 5 fold cross-validation : ", sum(boston_xgb_time), " mean rmse on test data : ", mean(boston_xgb_te), "\n")
NUM_FOLDS = 5
set.seed(1)
FOLDS = regr_folds(folds = NUM_FOLDS, Boston[, 'medv'], stratified = T)
boston_rgf_te = boston_ranger_te = boston_xgb_te = boston_rgf_time = boston_ranger_time = boston_xgb_time = rep(NA, NUM_FOLDS)
for (i in 1:length(FOLDS)) {
cat("fold : ", i, "\n")
samp = unlist(FOLDS[-i])
samp_ = unlist(FOLDS[i])
# RGF
#----
rgf_start = Sys.time()
init_regr = RGF_Regressor$new(l2 = 0.1)
init_regr$fit(x = as.matrix(Boston[samp, -ncol(Boston)]), y = Boston[samp, ncol(Boston)])
pr_te = init_regr$predict(as.matrix(Boston[samp_, -ncol(Boston)]))
rgf_end = Sys.time()
boston_rgf_time[i] = rgf_end - rgf_start
boston_rgf_te[i] = MLmetrics::RMSE(Boston[samp_, 'medv'], pr_te)
# ranger
#-------
ranger_start = Sys.time()
fit = ranger(dependent.variable.name = "medv", data = Boston[samp, ], write.forest = TRUE, probability = F, num.threads = 1,
num.trees = 500, verbose = T, classification = F, mtry = NULL, min.node.size = 5, keep.inbag = T)
pred_te = predict(fit, data = Boston[samp_, -ncol(Boston)], type = 'se')$predictions
ranger_end = Sys.time()
boston_ranger_time[i] = ranger_end - ranger_start
boston_ranger_te[i] = MLmetrics::RMSE(Boston[samp_, 'medv'], pred_te)
# xgboost
#--------
xgb_start = Sys.time()
dtrain <- xgb.DMatrix(data = as.matrix(Boston[samp, -ncol(Boston)]), label = Boston[samp, ncol(Boston)])
dtest <- xgb.DMatrix(data = as.matrix(Boston[samp_, -ncol(Boston)]), label = Boston[samp_, ncol(Boston)])
watchlist <- list(train = dtrain, test = dtest)
param = list("objective" = "reg:linear", "bst:eta" = 0.05, "max_depth" = 4, "subsample" = 0.85,
"colsample_bytree" = 0.85, "booster" = "gbtree",  "nthread" = 1)
fit = xgb.train(param, dtrain, nround = 500, print_every_n  = 100, watchlist = watchlist,
early_stopping_rounds = 20, maximize = FALSE, verbose = 0)
p_te = xgboost:::predict.xgb.Booster(fit, as.matrix(Boston[samp_, -ncol(Boston)]), ntreelimit = fit$best_iteration)
xgb_end = Sys.time()
boston_xgb_time[i] = xgb_end - xgb_start
boston_xgb_te[i] = MLmetrics::RMSE(Boston[samp_, 'medv'], p_te)
}
# time and error-rate
cat("total time rgf 5 fold cross-validation : ", sum(boston_rgf_time), " mean rmse on test data : ", mean(boston_rgf_te), "\n")
cat("total time ranger 5 fold cross-validation : ", sum(boston_ranger_time), " mean rmse on test data : ", mean(boston_ranger_te), "\n")
cat("total time xgb 5 fold cross-validation : ", sum(boston_xgb_time), " mean rmse on test data : ", mean(boston_xgb_te), "\n")
library(RGF)
# conversion from a matrix object a scipy sparse matrix
#------------------------------------------------------
set.seed(1)
x = matrix(runif(1000), nrow = 100, ncol = 10)
res = mat_2scipy_sparse(x)
print(dim(x))
print(res$shape)
# conversion from a dgCMatrix object a scipy sparse matrix
#----------------------------------------------------------
data = c(1, 0, 2, 0, 0, 3, 4, 5, 6)
dgcM = Matrix::Matrix(data = data, nrow = 3,
ncol = 3, byrow = TRUE,
sparse = TRUE)
print(dim(dgcM))
res = dgCMatrix_2scipy_sparse(dgcM)
print(res$shape)
data(Boston, package = 'KernelKnn')
library(RGF)
library(ranger)
library(xgboost)
# shuffling function for cross-validation folds
#-----------------------------------------------
func_shuffle = function(vec, times = 10) {
for (i in 1:times) {
out = sample(vec, length(vec))
}
out
}
# regression cross-validation folds
#-----------------------------------
regr_folds = function(folds, RESP, stratified = FALSE) {
if (is.factor(RESP)) {
stop(simpleError("this function is meant for regression for classification use the 'class_folds' function"))
}
samp_vec = rep(1/folds, folds)
sort_names = paste0('fold_', 1:folds)
if (stratified == TRUE) {
stratif = cut(RESP, breaks = folds)
clas = lapply(unique(stratif), function(x) which(stratif == x))
len = lapply(clas, function(x) length(x))
prop = lapply(len, function(y) sapply(1:length(samp_vec), function(x) round(y * samp_vec[x])))
repl = unlist(lapply(prop, function(x) sapply(1:length(x), function(y) rep(paste0('fold_', y), x[y]))))
spl = suppressWarnings(split(1:length(RESP), repl))}
else {
prop = lapply(length(RESP), function(y) sapply(1:length(samp_vec), function(x) round(y * samp_vec[x])))
repl = func_shuffle(unlist(lapply(prop, function(x) sapply(1:length(x), function(y) rep(paste0('fold_', y), x[y])))))
spl = suppressWarnings(split(1:length(RESP), repl))
}
spl = spl[sort_names]
if (length(table(unlist(lapply(spl, function(x) length(x))))) > 1) {
warning('the folds are not equally split')
}
if (length(unlist(spl)) != length(RESP)) {
stop(simpleError("the length of the splits are not equal with the length of the response"))
}
spl
}
data(Boston, package = 'KernelKnn')
library(RGF)
library(ranger)
library(xgboost)
# shuffling function for cross-validation folds
#-----------------------------------------------
func_shuffle = function(vec, times = 10) {
for (i in 1:times) {
out = sample(vec, length(vec))
}
out
}
# regression cross-validation folds
#-----------------------------------
regr_folds = function(folds, RESP, stratified = FALSE) {
if (is.factor(RESP)) {
stop(simpleError("this function is meant for regression for classification use the 'class_folds' function"))
}
samp_vec = rep(1/folds, folds)
sort_names = paste0('fold_', 1:folds)
if (stratified == TRUE) {
stratif = cut(RESP, breaks = folds)
clas = lapply(unique(stratif), function(x) which(stratif == x))
len = lapply(clas, function(x) length(x))
prop = lapply(len, function(y) sapply(1:length(samp_vec), function(x) round(y * samp_vec[x])))
repl = unlist(lapply(prop, function(x) sapply(1:length(x), function(y) rep(paste0('fold_', y), x[y]))))
spl = suppressWarnings(split(1:length(RESP), repl))}
else {
prop = lapply(length(RESP), function(y) sapply(1:length(samp_vec), function(x) round(y * samp_vec[x])))
repl = func_shuffle(unlist(lapply(prop, function(x) sapply(1:length(x), function(y) rep(paste0('fold_', y), x[y])))))
spl = suppressWarnings(split(1:length(RESP), repl))
}
spl = spl[sort_names]
if (length(table(unlist(lapply(spl, function(x) length(x))))) > 1) {
warning('the folds are not equally split')
}
if (length(unlist(spl)) != length(RESP)) {
stop(simpleError("the length of the splits are not equal with the length of the response"))
}
spl
}
NUM_FOLDS = 5
set.seed(1)
FOLDS = regr_folds(folds = NUM_FOLDS, Boston[, 'medv'], stratified = T)
boston_rgf_te = boston_ranger_te = boston_xgb_te = boston_rgf_time = boston_ranger_time = boston_xgb_time = rep(NA, NUM_FOLDS)
for (i in 1:length(FOLDS)) {
cat("fold : ", i, "\n")
samp = unlist(FOLDS[-i])
samp_ = unlist(FOLDS[i])
# RGF
#----
rgf_start = Sys.time()
init_regr = RGF_Regressor$new(l2 = 0.1)
init_regr$fit(x = as.matrix(Boston[samp, -ncol(Boston)]), y = Boston[samp, ncol(Boston)])
pr_te = init_regr$predict(as.matrix(Boston[samp_, -ncol(Boston)]))
rgf_end = Sys.time()
boston_rgf_time[i] = rgf_end - rgf_start
boston_rgf_te[i] = MLmetrics::RMSE(Boston[samp_, 'medv'], pr_te)
# ranger
#-------
ranger_start = Sys.time()
fit = ranger(dependent.variable.name = "medv", data = Boston[samp, ], write.forest = TRUE, probability = F, num.threads = 1,
num.trees = 500, verbose = T, classification = F, mtry = NULL, min.node.size = 5, keep.inbag = T)
pred_te = predict(fit, data = Boston[samp_, -ncol(Boston)], type = 'se')$predictions
ranger_end = Sys.time()
boston_ranger_time[i] = ranger_end - ranger_start
boston_ranger_te[i] = MLmetrics::RMSE(Boston[samp_, 'medv'], pred_te)
# xgboost
#--------
xgb_start = Sys.time()
dtrain <- xgb.DMatrix(data = as.matrix(Boston[samp, -ncol(Boston)]), label = Boston[samp, ncol(Boston)])
dtest <- xgb.DMatrix(data = as.matrix(Boston[samp_, -ncol(Boston)]), label = Boston[samp_, ncol(Boston)])
watchlist <- list(train = dtrain, test = dtest)
param = list("objective" = "reg:linear", "bst:eta" = 0.05, "max_depth" = 4, "subsample" = 0.85,
"colsample_bytree" = 0.85, "booster" = "gbtree",  "nthread" = 1)
fit = xgb.train(param, dtrain, nround = 500, print_every_n  = 100, watchlist = watchlist,
early_stopping_rounds = 20, maximize = FALSE, verbose = 0)
p_te = xgboost:::predict.xgb.Booster(fit, as.matrix(Boston[samp_, -ncol(Boston)]), ntreelimit = fit$best_iteration)
xgb_end = Sys.time()
boston_xgb_time[i] = xgb_end - xgb_start
boston_xgb_te[i] = MLmetrics::RMSE(Boston[samp_, 'medv'], p_te)
}
cat("total time rgf 5 fold cross-validation : ", sum(boston_rgf_time), " mean rmse on test data : ", mean(boston_rgf_te), "\n")
cat("total time ranger 5 fold cross-validation : ", sum(boston_ranger_time), " mean rmse on test data : ", mean(boston_ranger_te), "\n")
cat("total time xgb 5 fold cross-validation : ", sum(boston_xgb_time), " mean rmse on test data : ", mean(boston_xgb_te), "\n")
train_dat = read.table(unz("africa_soil_train_data.zip", "train.csv"), nrows = 1157, header = T, quote = "\"", sep = ",")
getwd()
setwd("~/Downloads")
train_dat = read.table(unz("africa_soil_train_data.zip", "train.csv"), nrows = 1157, header = T, quote = "\"", sep = ",")
x = train_dat[, -c(1, which(colnames(train_dat) %in% c("Ca", "P", "pH", "SOC", "Sand", "Depth")))]
# take (randomly) the first of the responses for train
y = train_dat[, "Ca"]
# data set for ranger
tmp_rg_dat = cbind(Ca = y, x)
# cross-validation folds
set.seed(2)
FOLDS = regr_folds(folds = NUM_FOLDS, y, stratified = T)
highdim_rgf_te = highdim_ranger_te = highdim_xgb_te = highdim_rgf_time = highdim_ranger_time = highdim_xgb_time = rep(NA, NUM_FOLDS)
for (i in 1:length(FOLDS)) {
cat("fold : ", i, "\n")
new_samp = unlist(FOLDS[-i])
new_samp_ = unlist(FOLDS[i])
# RGF
#----
rgf_start = Sys.time()
init_regr = FastRGF_Regressor$new(n_jobs = 5, l2 = 0.1)              # added 'l2' regularization
init_regr$fit(x = as.matrix(x[new_samp, ]), y = y[new_samp])
pr_te = init_regr$predict(as.matrix(x[new_samp_, ]))
rgf_end = Sys.time()
highdim_rgf_time[i] = rgf_end - rgf_start
highdim_rgf_te[i] = MLmetrics::RMSE(y[new_samp_], pr_te)
# ranger
#-------
ranger_start = Sys.time()
fit = ranger(dependent.variable.name = "Ca", data = tmp_rg_dat[new_samp, ], write.forest = TRUE, probability = F, num.threads = 5,
num.trees = 500, verbose = T, classification = F, mtry = NULL, min.node.size = 5, keep.inbag = T)
pred_te = predict(fit, data = x[new_samp_, ], type = 'se')$predictions
ranger_end = Sys.time()
highdim_ranger_time[i] = ranger_end - ranger_start
highdim_ranger_te[i] = MLmetrics::RMSE(y[new_samp_], pred_te)
# xgboost
#--------
xgb_start = Sys.time()
dtrain <- xgb.DMatrix(data = as.matrix(x[new_samp, ]), label = y[new_samp])
dtest <- xgb.DMatrix(data = as.matrix(x[new_samp_, ]), label = y[new_samp_])
watchlist <- list(train = dtrain, test = dtest)
param = list("objective" = "reg:linear", "bst:eta" = 0.05, "max_depth" = 6, "subsample" = 0.85,
"colsample_bytree" = 0.85, "booster" = "gbtree", "nthread" = 5)                        # by adding "lambda" = 0.1 RMSE is not improved
fit = xgb.train(param, dtrain, nround = 500, print_every_n  = 100, watchlist = watchlist,
early_stopping_rounds = 20, maximize = FALSE, verbose = 0)
p_te = xgboost:::predict.xgb.Booster(fit, as.matrix(x[new_samp_, ]), ntreelimit = fit$best_iteration)
xgb_end = Sys.time()
highdim_xgb_time[i] = xgb_end - xgb_start
highdim_xgb_te[i] = MLmetrics::RMSE(y[new_samp_], p_te)
}
cat("total time rgf 5 fold cross-validation : ", sum(highdim_rgf_time), " mean rmse on test data : ", mean(highdim_rgf_te), "\n")
cat("total time ranger 5 fold cross-validation : ", sum(highdim_ranger_time), " mean rmse on test data : ", mean(highdim_ranger_te), "\n")
cat("total time xgb 5 fold cross-validation : ", sum(highdim_xgb_time), " mean rmse on test data : ", mean(highdim_xgb_te), "\n")
getwd()
devtools::document(pkg = '/home/lampros/ADD_GITHUB/RGF')
getwd()
devtools::document(pkg = '/home/lampros/ADD_GITHUB/RGF')
setwd('/home/lampros/ADD_GITHUB/')
system("R CMD build RGF")
system("R CMD check --as-cran RGF_1.0.0.tar.gz")
remove.packages("RGF", lib="~/R/x86_64-pc-linux-gnu-library/3.4")
system("R CMD INSTALL RGF_1.0.0.tar.gz")
library("RGF", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
detach("package:RGF", unload=TRUE)
res = jsonlite::fromJSON("https://www.kaggle.com/competitions.json", flatten = T)
res
res = jsonlite::fromJSON("https://www.kaggle.com/competitions.json", flatten = F)
str(res)
str(res$fullCompetitionGroups)
res1 = geojsonR::shiny_from_JSON("https://www.kaggle.com/competitions.json")
str(res1)
res = jsonlite::fromJSON("https://www.kaggle.com/competitions.json", simplifyVector = F, simplifyDataFrame = F, simplifyMatrix = F, flatten = F)
str(res)
str(res)
names(res)
str(res$categoryOptions)
str(res$fullCompetitionGroups)
tmp = res$fullCompetitionGroups[[2]])
tmp = res$fullCompetitionGroups[[2]]
tmp = str(res$fullCompetitionGroups[[2]])
str(res$fullCompetitionGroups)
res1 = geojsonR::FROM_GeoJson_Schema("https://www.kaggle.com/competitions.json")
str(res1)
str(res1$fullCompetitionGroups[[2]])
active_competitions = res1$fullCompetitionGroups[[2]]                     # list [[1]] is an empty list [ entered competitions is 0 as I'm not signed in ]
kaggle_json = geojsonR::FROM_GeoJson_Schema("https://www.kaggle.com/competitions.json")
kaggle_json = geojsonR::FROM_GeoJson_Schema("https://www.kaggle.com/competitions.json")
active_competitions = kaggle_json$fullCompetitionGroups[[2]]                     # list [[1]] is an empty list [ entered competitions is 0 as I'm not signed in ]
install.packages("microbenchmark")
microbenchmark::microbenchmark(jsonlite::fromJSON("https://www.kaggle.com/competitions.json", simplifyVector = F, simplifyDataFrame = F, simplifyMatrix = F, flatten = F),
geojsonR::FROM_GeoJson_Schema("https://www.kaggle.com/competitions.json"), times = 10)
length(active_competitions)
str(active_competitions)
length(active_competitions$competitions)
active_competitions = kaggle_json$fullCompetitionGroups[[2]]$competitions                                      # list [[1]] is an empty list [ entered competitions is 0 as I'm not signed in ]
length(active_competitions)
str(active_competitions)
active_competitions[[1]]
tmp = active_competitions[[1]]
tmp$awardsPoints
tmp$competitionDescription
tmp$enabledDate
list(title = tmp$competitionTitle, description = tmp$competitionDescription, start_date = tmp$enabledDate, awards_points = tmp$awardsPoints)
do.call(rbind, list(title = tmp$competitionTitle, description = tmp$competitionDescription, start_date = tmp$enabledDate, awards_points = tmp$awardsPoints))
do.call(cbind, list(title = tmp$competitionTitle, description = tmp$competitionDescription, start_date = tmp$enabledDate, awards_points = tmp$awardsPoints))
tmp$enabledDate
as.Date(tmp$enabledDate)
tmp_date = as.Date(tmp$enabledDate)
tmp_date
str(tmp_date)
date()
?date
str(date())
str(as.Date(date()))
as.Date(date())
format(Sys.time(), "%a %b %d %H:%M:%S %Y")
format(Sys.time(), "%d %H:%M:%S %Y")
format(Sys.time(), "%d %m %Y")
format(Sys.time(), "%d-%m-%Y")
format(Sys.time(), "%Y-%m-%d")
str(tmp_date)
format(Sys.time(), "%Y-%m-%d")
tmp_date
class(format(Sys.time(), "%Y-%m-%d"))
as.Date(format(Sys.time(), "%Y-%m-%d"))
str(as.Date(format(Sys.time(), "%Y-%m-%d")))
as.Date(tmp$enabledDate)
do.call(rbind, lapply(active_competitions, function(x) {
tmp_date = as.Date(tmp$enabledDate)
do.call(cbind, list(title = tmp$competitionTitle, description = tmp$competitionDescription, start_date = tmp_date, awards_points = tmp$awardsPoints))
}))
length(active_competitions)
do.call(rbind, lapply(active_competitions, function(x) {
tmp_date = as.Date(x$enabledDate)
do.call(cbind, list(title = x$competitionTitle, description = x$competitionDescription, start_date = tmp_date, awards_points = x$awardsPoints))
}))
do.call(rbind, lapply(active_competitions, function(x) {
tmp_date = as.Date(x$enabledDate)
data.frame(do.call(cbind, list(title = x$competitionTitle, description = x$competitionDescription, start_date = tmp_date, awards_points = x$awardsPoints)))
}))
do.call(rbind, lapply(active_competitions, function(x) {
#tmp_date = as.Date(x$enabledDate)
tmp_date = x$enabledDate
data.frame(do.call(cbind, list(title = x$competitionTitle, description = x$competitionDescription, start_date = tmp_date, awards_points = x$awardsPoints)))
}))
do.call(rbind, lapply(active_competitions, function(x) {
#tmp_date = as.Date(x$enabledDate)
tmp_date = x$enabledDate
do.call(cbind, list(title = x$competitionTitle, description = x$competitionDescription, start_date = tmp_date, awards_points = x$awardsPoints))
}))
df = do.call(rbind, lapply(active_competitions, function(x) {
#tmp_date = as.Date(x$enabledDate)
tmp_date = x$enabledDate
do.call(cbind, list(title = x$competitionTitle, description = x$competitionDescription, start_date = tmp_date, awards_points = x$awardsPoints))
}))
df
df = data.frame(do.call(rbind, lapply(active_competitions, function(x) {
#tmp_date = as.Date(x$enabledDate)
tmp_date = x$enabledDate
do.call(cbind, list(title = x$competitionTitle, description = x$competitionDescription, start_date = tmp_date, awards_points = x$awardsPoints))
})))
df
df$start_date
?data.frame
df$start_date = as.Date(as.character(df$start_date))
df
dat = as.Date("2018-01-16")
dat
dat = as.Date("2018-02-12")
dat
as.Date(format(Sys.time(), "%Y-%m-%d"))
dat1 = as.Date(format(Sys.time(), "%Y-%m-%d"))
dat1
dat == dat1
df
df[order(df$start_date, decreasing = F), ]
df[order(df$start_date, decreasing = T), ]
ordered_df = df[order(df$start_date, decreasing = T), ]
ordered_df
ordered_df$start_date[1]
ordered_df$start_date[1] == as.Date(format(Sys.time(), "%Y-%m-%d"))
as.Date(format(Sys.time(), "%Y-%m-%d"))
ordered_df$start_date[1]
str(ordered_df$start_date[1])
str(as.Date(format(Sys.time(), "%Y-%m-%d")))
install.packages("rvest")
library('rvest')
URL = 'https://stackoverflow.com/search?q=%5Br%5D%5Bopenimager'
webpage <- read_html(url)
webpage <- read_html(URL)
webpage
str(webpage)
webpage$node
?read_html
library(jsonlite)
library(httr)
url <- "https://stackoverflow.com/search?q=%5Br%5D%5Bopenimager"
a <- GET(url)
a <- content(a, as="text")
a
length(a)
a
a
library(jsonlite)
library(httr)
url <- "https://stackoverflow.com/search?q=%5Br%5D%5Bopenimager"
a <- GET(url)
output<- content(a, as="text") %>% fromJSON(flatten=FALSE)
library(jsonlite)
library(httr)
url <- "https://stackoverflow.com/search?q=%5Br%5D%5Bopenimager"
url_get <- GET(url)
raw_text <- content(url_get, as="text")
results <- raw_text %>% html_nodes(".short-desc")
url <- "https://stackoverflow.com/search?q=%5Br%5D%5Bopenimager"
library(rvest)
webpage <- read_html(url)
webpage
results <- webpage %>% html_nodes(".short-desc")
results
remove.packages("RGF", lib="~/R/x86_64-pc-linux-gnu-library/3.4")
